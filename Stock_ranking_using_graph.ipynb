{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stock_ranking_using_graph.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMwYlH+4BveknGsNALHPMIp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omkar-salunke/Trading_algos/blob/main/Stock_ranking_using_graph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UpzGm0xZvz1",
        "outputId": "92a99c99-5ba7-447b-8858-9a1e390adf7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stock-ranking-using-list-wise-approach'...\n",
            "remote: Enumerating objects: 171, done.\u001b[K\n",
            "remote: Counting objects: 100% (171/171), done.\u001b[K\n",
            "remote: Compressing objects: 100% (160/160), done.\u001b[K\n",
            "remote: Total 171 (delta 47), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (171/171), 2.59 MiB | 4.70 MiB/s, done.\n",
            "Resolving deltas: 100% (47/47), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/sumansaha66/stock-ranking-using-list-wise-approach.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# /content/stock-ranking-using-list-wise-approach/training\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/stock-ranking-using-list-wise-approach/training')"
      ],
      "metadata": {
        "id": "BpipmKt7aRP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import scipy.stats as sps\n",
        "from rbo import rbo_at_k_normalised_w\n",
        "\n",
        "def bt_long_calculator(pre_topn, ground_truth, bt_longn, i):\n",
        "    # back testing on top k stocks\n",
        "    real_ret_rat_topn = 0\n",
        "    for pre in pre_topn:\n",
        "        real_ret_rat_topn += ground_truth[pre][i]\n",
        "    real_ret_rat_topn /= len(pre_topn)\n",
        "    bt_longn += real_ret_rat_topn\n",
        "    return bt_longn\n",
        "\n",
        "def evaluate(prediction, ground_truth, mask, report=False):\n",
        "    assert ground_truth.shape == prediction.shape, 'shape mis-match'\n",
        "    # Performance is the dictionary which will contain the mse, mrrt and btl\n",
        "    performance = {}\n",
        "    # calculation of mse. this is equivalent to reg_loss or regression loss\n",
        "    performance['mse'] = np.linalg.norm((prediction - ground_truth) * mask)**2\\\n",
        "        / np.sum(mask)\n",
        "    mrr_top = 0.0\n",
        "    all_miss_days_top = 0\n",
        "    bt_long = 1.0\n",
        "    bt_long5 = 1.0\n",
        "    bt_long10 = 1.0\n",
        "    bt_long20 = 1.0\n",
        "    bt_long50 = 1.0\n",
        "    rbo_at_5_normalised=0.0\n",
        "    rbo_at_10_normalised=0.0\n",
        "    rbo_at_20_normalised=0.0\n",
        "    rbo_at_50_normalised=0.0\n",
        "\n",
        "    for i in range(prediction.shape[1]):\n",
        "        # prediction.shape[1] is the number of days\n",
        "        # This loop will iterate over the length of test and validation set\n",
        "        # Actual rank based on ground truth\n",
        "        rank_gt = np.argsort(ground_truth[:, i])\n",
        "        gt_top1 = [] # will contain index of the top 1 stock by actual return\n",
        "        gt_top5 = [] # will contain index of the top 5 stock by actual return\n",
        "        gt_top10 = [] # will contain  index of the top 10 stock by actual return\n",
        "        gt_top20 = [] # will contain index of the top 20 stock by actual return\n",
        "        gt_top50 = [] # will contain index of the top 50 stock by actual return\n",
        "        \n",
        "        # Creasting list of top 1, 5, 10, 20 and 50 based on actual rank\n",
        "        for j in range(1, prediction.shape[0] + 1):\n",
        "            # This loop will iterate over the number of stocks (1 to 1026)\n",
        "            cur_rank = rank_gt[-1 * j] # Actual rank\n",
        "            if mask[cur_rank][i] < 0.5:\n",
        "                continue\n",
        "            if len(gt_top1) < 1:\n",
        "                gt_top1.append(cur_rank) # index of the top 1 stock by actual return\n",
        "            if len(gt_top5) < 5:\n",
        "                gt_top5.append(cur_rank) # index of the top 5 stock by actual return\n",
        "            if len(gt_top10) < 10:\n",
        "                gt_top10.append(cur_rank) # index of the top 10 stock by actual return\n",
        "            if len(gt_top20) < 20:\n",
        "                gt_top20.append(cur_rank) # index of the top 20 stock by actual return\n",
        "            if len(gt_top50) < 50:\n",
        "                gt_top50.append(cur_rank) # index of the top 50 stock by actual return\n",
        "\n",
        "        # Predicted Rank\n",
        "        rank_pre = np.argsort(prediction[:, i])\n",
        "\n",
        "        pre_top1 = [] # index of the top 1 stock by predicted return\n",
        "        pre_top5 = [] # index of the top 5 stock by predicted return\n",
        "        pre_top10 = [] # index of the top 10 stock by predicted return\n",
        "        pre_top20 = [] # index of the top 20 stock by predicted return\n",
        "        pre_top50 = [] # index of the top 50 stock by predicted return\n",
        "        for j in range(1, prediction.shape[0] + 1):\n",
        "            # This loop will iterate over the number of stocks (1 to num_company)\n",
        "            cur_rank = rank_pre[-1 * j]\n",
        "            if mask[cur_rank][i] < 0.5:\n",
        "                continue\n",
        "            if len(pre_top1) < 1:\n",
        "                pre_top1.append(cur_rank) # index of the top 1 stock by predicted return\n",
        "            if len(pre_top5) < 5:\n",
        "                pre_top5.append(cur_rank) # index of the top 5 stock by predicted return\n",
        "            if len(pre_top10) < 10:\n",
        "                pre_top10.append(cur_rank) # index of the top 10 stock by predicted return\n",
        "            if len(pre_top20) < 20:\n",
        "                pre_top20.append(cur_rank) # index of the top 20 stock by predicted return\n",
        "            if len(pre_top50) < 50:\n",
        "                pre_top50.append(cur_rank) # index of the top 50 stock by predicted return\n",
        "\n",
        "        # calculate mrr of top1\n",
        "        top1_pos_in_gt = 0\n",
        "        for j in range(1, prediction.shape[0] + 1):\n",
        "            # This loop will iterate over the number of stocks (1 to num_company)\n",
        "            cur_rank = rank_gt[-1 * j]\n",
        "            if mask[cur_rank][i] < 0.5:\n",
        "                continue\n",
        "            else:\n",
        "                # top1_pos_in_gt will calculate the rank of the predicted top stock\n",
        "                # in actual ground truth\n",
        "                top1_pos_in_gt += 1\n",
        "                if cur_rank in pre_top1:\n",
        "                    break\n",
        "        if top1_pos_in_gt == 0:\n",
        "            all_miss_days_top += 1\n",
        "        else:\n",
        "            # mrr_top will contain sum over all days/length of validation and\n",
        "            # test set\n",
        "            mrr_top += 1.0 / top1_pos_in_gt\n",
        "\n",
        "        # back testing on top 1 to calculate IRR\n",
        "        real_ret_rat_top = ground_truth[(pre_top1)[0]][i]\n",
        "        bt_long += real_ret_rat_top\n",
        "\n",
        "        # back testing\n",
        "        bt_long5= bt_long_calculator(pre_top5, ground_truth, bt_long5, i) # back testing on top 5        \n",
        "        bt_long10= bt_long_calculator(pre_top10, ground_truth, bt_long10, i) # back testing on top 10\n",
        "        bt_long20= bt_long_calculator(pre_top20, ground_truth, bt_long20, i) # back testing on top 20\n",
        "        bt_long50= bt_long_calculator(pre_top50, ground_truth, bt_long50, i) # back testing on top 50\n",
        "        \n",
        "        # nrbo calculation\n",
        "        rbo_at_5_normalised+=rbo_at_k_normalised_w(pre_top5,gt_top5,p=0.80, depth=5)\n",
        "        rbo_at_10_normalised+=rbo_at_k_normalised_w(pre_top5,gt_top5,p=0.90, depth=10)\n",
        "        rbo_at_20_normalised+=rbo_at_k_normalised_w(pre_top5,gt_top5,p=0.95,depth=20)\n",
        "        rbo_at_50_normalised+=rbo_at_k_normalised_w(pre_top5,gt_top5,p=0.98, depth=50)\n",
        "\n",
        "\n",
        "    performance['mrrt'] = mrr_top / (prediction.shape[1] - all_miss_days_top)\n",
        "    performance['rbo_at_5_normalized'] = rbo_at_5_normalised / (prediction.shape[1])\n",
        "    performance['rbo_at_10_normalized'] = rbo_at_10_normalised / (prediction.shape[1])\n",
        "    performance['rbo_at_20_normalized'] = rbo_at_20_normalised / (prediction.shape[1])\n",
        "    performance['rbo_at_50_normalized'] = rbo_at_50_normalised / (prediction.shape[1])\n",
        "    performance['btl'] = bt_long\n",
        "    performance['bt5_unweighted'] = bt_long5\n",
        "    performance['bt10_unweighted'] = bt_long10\n",
        "    performance['bt20_unweighted'] = bt_long20\n",
        "    performance['bt50_unweighted'] = bt_long50\n",
        "    return performance\n",
        "\n",
        "def make_df_loss(rr_lstm,epoch,cur_valid_perf,cur_test_perf,tra_loss,tra_reg_loss,val_loss,test_loss):\n",
        "    loss_df=rr_lstm.df_loss.append({\n",
        "                        'epoch':epoch,\n",
        "                        'market':rr_lstm.market_name,\n",
        "                        'relation_name':rr_lstm.relation_name,\n",
        "                        'loss_name':rr_lstm.loss_name,\n",
        "                        'train_total_loss':tra_loss.numpy() / (rr_lstm.valid_index - rr_lstm.parameters['seq'] - rr_lstm.steps + 1),\n",
        "                        'train_reg_loss':tra_reg_loss.numpy() / (rr_lstm.valid_index - rr_lstm.parameters['seq'] - rr_lstm.steps + 1),\n",
        "                        'valid_total_loss':val_loss.numpy()  / (rr_lstm.test_index - rr_lstm.valid_index),\n",
        "                        'valid_reg_loss':cur_valid_perf['mse'],'valid_mrrt':cur_valid_perf['mrrt'],\n",
        "                        'valid_bt1':cur_valid_perf['btl'],'valid_bt5_unweighted':cur_valid_perf['bt5_unweighted'],\n",
        "                        'valid_bt10_unweighted':cur_valid_perf['bt10_unweighted'],\n",
        "                        'valid_bt20_unweighted':cur_valid_perf['bt20_unweighted'],\n",
        "                        'valid_bt50_unweighted':cur_valid_perf['bt50_unweighted'],\n",
        "                        'valid_rbo_at_5_normalized':cur_valid_perf['rbo_at_5_normalized'],\n",
        "                        'valid_rbo_at_10_normalized':cur_valid_perf['rbo_at_10_normalized'],\n",
        "                        'valid_rbo_at_20_normalized':cur_valid_perf['rbo_at_20_normalized'],\n",
        "                        'valid_rbo_at_50_normalized':cur_valid_perf['rbo_at_50_normalized'],\n",
        "                        'test_total_loss':test_loss.numpy() / (rr_lstm.trade_dates - rr_lstm.test_index),\n",
        "                        'test_reg_loss':cur_test_perf['mse'],'test_mrrt':cur_test_perf['mrrt'],\n",
        "                        'test_bt1':cur_test_perf['btl'],'test_bt5_unweighted':cur_test_perf['bt5_unweighted'],\n",
        "                        'test_bt10_unweighted':cur_test_perf['bt10_unweighted'],\n",
        "                        'test_bt20_unweighted':cur_test_perf['bt20_unweighted'],\n",
        "                        'test_bt50_unweighted':cur_test_perf['bt50_unweighted'],\n",
        "                        'test_rbo_at_5_normalized':cur_test_perf['rbo_at_5_normalized'],\n",
        "                        'test_rbo_at_10_normalized':cur_test_perf['rbo_at_10_normalized'],\n",
        "                        'test_rbo_at_20_normalized':cur_test_perf['rbo_at_20_normalized'],\n",
        "                        'test_rbo_at_50_normalized':cur_test_perf['rbo_at_50_normalized']},\n",
        "                    ignore_index=True)\n",
        "    return loss_df"
      ],
      "metadata": {
        "id": "kmVqVFwbZy7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "# Used for loading sequential embedding\n",
        "def load_EOD_data(data_path, market_name, tickers, steps=1):\n",
        "    eod_data = []\n",
        "    masks = []\n",
        "    ground_truth = []\n",
        "    base_price = []\n",
        "    # Go through all the tickers one by one\n",
        "    for index, ticker in enumerate(tickers):\n",
        "        # Load raw data of each ticker. There are six columns when loading from 2013-01-01 folder.\n",
        "        # column[0] index or time\n",
        "        # column[1] most likely 5 day average of normalized price\n",
        "        # column[2]: most likely 10 day average of normalized price\n",
        "        # column [3]: most likely 20 day average of normalized price\n",
        "        # column [4]: most likely 30 day average of normalized price\n",
        "        # column [5]: most likely the normalized price\n",
        "        # The length is 1245 which represents total number of days from 2013-2017\n",
        "        single_EOD = np.genfromtxt(\n",
        "            os.path.join(data_path, market_name + '_' + ticker + '_1.csv'),\n",
        "            dtype=np.float32, delimiter=',', skip_header=False\n",
        "        )\n",
        "        if market_name == 'NASDAQ':\n",
        "            # remove the last day since lots of missing data\n",
        "            single_EOD = single_EOD[:-1, :]\n",
        "        if index == 0:\n",
        "            # Print the length of the overall time series\n",
        "            print('single EOD data shape:', single_EOD.shape)\n",
        "            # tensor of time series data\n",
        "            eod_data = np.zeros([len(tickers), single_EOD.shape[0],\n",
        "                                 single_EOD.shape[1] - 1], dtype=np.float32) # (num_company,num_days,5)\n",
        "            # Initially all the masks will be 1\n",
        "            masks = np.ones([len(tickers), single_EOD.shape[0]],\n",
        "                            dtype=np.float32) # (num_company,num_days)\n",
        "            \n",
        "            ground_truth = np.zeros([len(tickers), single_EOD.shape[0]],\n",
        "                                    dtype=np.float32) # (num_company,num_days)\n",
        "            # Take the price of all stocks\n",
        "            base_price = np.zeros([len(tickers), single_EOD.shape[0]],\n",
        "                                  dtype=np.float32) # (num_company,num_days)\n",
        "        for row in range(single_EOD.shape[0]): # for each day range(0, num_days)\n",
        "            # Calculate return or ground truth\n",
        "            if abs(single_EOD[row][-1] + 1234) < 1e-8:\n",
        "                # This is most likely to deal with missing data. If any day is \n",
        "                # missing for a stock, mask for that day will be 0. Raw data of\n",
        "                # that day is -1234 in the raw/individual file\n",
        "                masks[index][row] = 0.0\n",
        "            elif row > steps - 1 and abs(single_EOD[row - steps][-1] + 1234)> 1e-8:\n",
        "                # Return is calculated as ground truth\n",
        "                ground_truth[index][row] =(single_EOD[row][-1] - single_EOD[row - steps][-1]) /single_EOD[row - steps][-1]\n",
        "            for col in range(single_EOD.shape[1]):\n",
        "                if abs(single_EOD[row][col] + 1234) < 1e-8:\n",
        "                    single_EOD[row][col] = 1.1\n",
        "        eod_data[index, :, :] = single_EOD[:, 1:] # take all eod data except index\n",
        "        base_price[index, :] = single_EOD[:, -1] # Take the normalized price of all stocks. Last column of single_EOD\n",
        "    return eod_data, masks, ground_truth, base_price\n",
        "\n",
        "\n",
        "def load_graph_relation_data(relation_file, lap=False):\n",
        "    relation_encoding = np.load(relation_file) # (num_company, num_company, relation_types)\n",
        "    print('relation encoding shape:', relation_encoding.shape)\n",
        "    rel_shape = [relation_encoding.shape[0], relation_encoding.shape[1]]\n",
        "    mask_flags = np.equal(np.zeros(rel_shape, dtype=int),\n",
        "                          np.sum(relation_encoding, axis=2))\n",
        "    ajacent = np.where(mask_flags, np.zeros(rel_shape, dtype=float),\n",
        "                       np.ones(rel_shape, dtype=float))\n",
        "    degree = np.sum(ajacent, axis=0)\n",
        "    for i in range(len(degree)):\n",
        "        degree[i] = 1.0 / degree[i]\n",
        "    np.sqrt(degree, degree)\n",
        "    deg_neg_half_power = np.diag(degree)\n",
        "    if lap:\n",
        "        return np.identity(ajacent.shape[0], dtype=float) - np.dot(\n",
        "            np.dot(deg_neg_half_power, ajacent), deg_neg_half_power)\n",
        "    else:\n",
        "        return np.dot(np.dot(deg_neg_half_power, ajacent), deg_neg_half_power)\n",
        "\n",
        "# Used for loading relational data\n",
        "def load_relation_data(relation_file):\n",
        "    relation_encoding = np.load(relation_file) # Contains relation data (only 0 or 1)\n",
        "    print('relation encoding shape:', relation_encoding.shape) # (num_company, num_company, relation_type)\n",
        "    rel_shape = [relation_encoding.shape[0], relation_encoding.shape[1]] # (num_company, num_company)\n",
        "    # Sum all types of relations. mask_flags will be 1 if no relation exists between\n",
        "    # two companies\n",
        "    mask_flags = np.equal(np.zeros(rel_shape, dtype=int),\n",
        "                          np.sum(relation_encoding, axis=2)) # (num_company, num_company)\n",
        "    \n",
        "    mask = np.where(mask_flags, np.ones(rel_shape) * -1e9, np.zeros(rel_shape)) # If there is no relation the mask will be a large negative number there -1e9\n",
        "    return relation_encoding, mask # return individual and masked relations\n",
        "\n",
        "\n",
        "def build_SFM_data(data_path, market_name, tickers):\n",
        "    eod_data = []\n",
        "    for index, ticker in enumerate(tickers):\n",
        "        single_EOD = np.genfromtxt(\n",
        "            os.path.join(data_path, market_name + '_' + ticker + '_1.csv'),\n",
        "            dtype=np.float32, delimiter=',', skip_header=False\n",
        "        )\n",
        "        if index == 0:\n",
        "            print('single EOD data shape:', single_EOD.shape)\n",
        "            eod_data = np.zeros([len(tickers), single_EOD.shape[0]],\n",
        "                                dtype=np.float32)\n",
        "\n",
        "        for row in range(single_EOD.shape[0]):\n",
        "            if abs(single_EOD[row][-1] + 1234) < 1e-8:\n",
        "                # handle missing data\n",
        "                if row < 3:\n",
        "                    # eod_data[index, row] = 0.0\n",
        "                    for i in range(row + 1, single_EOD.shape[0]):\n",
        "                        if abs(single_EOD[i][-1] + 1234) > 1e-8:\n",
        "                            eod_data[index][row] = single_EOD[i][-1]\n",
        "                            # print(index, row, i, eod_data[index][row])\n",
        "                            break\n",
        "                else:\n",
        "                    eod_data[index][row] = np.sum(\n",
        "                        eod_data[index, row - 3:row]) / 3\n",
        "                    # print(index, row, eod_data[index][row])\n",
        "            else:\n",
        "                eod_data[index][row] = single_EOD[row][-1]\n",
        "        # print('test point')\n",
        "    np.save(market_name + '_sfm_data', eod_data)"
      ],
      "metadata": {
        "id": "k90mbQMdaBQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''This code is the test with inclusion of node2vec'''\n",
        "\n",
        "# Import packages\n",
        "import argparse\n",
        "import copy\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "from time import time \n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "from load_data import load_EOD_data, load_relation_data\n",
        "from evaluator import evaluate, make_df_loss\n",
        "from loss_functions_tgc import reg_loss_tgc, rank_loss_tgc, listnet_loss\n",
        "from graph_embedding import relation_node2vec\n",
        "\n",
        "# Set up random seeds\n",
        "seed = 123456789\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# Function for initialising arrays\n",
        "def prediction_ground_truth_mask_initializer(dim1,dim2):\n",
        "    pred_array = np.zeros([dim1, dim2],dtype=float)\n",
        "    gt_array=np.zeros([dim1, dim2],dtype=float)\n",
        "    mask_array=np.zeros([dim1, dim2],dtype=float)\n",
        "    return pred_array, gt_array, mask_array\n",
        "\n",
        "\n",
        "\n",
        "# Another model for 'inner product weight' can be similarly built\n",
        "class MyModel(Model):\n",
        "  def __init__(self, nCom, rel_mask, inner_prod, flat,rel_encoding,num_random_walks,len_random_walk,\n",
        "               p_val,q_val,n2vemb_size,units = 0):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.rel_mask = rel_mask\n",
        "    self.inner_prod = inner_prod\n",
        "    self.flat = flat\n",
        "    self.all_one = tf.ones([nCom, 1], dtype=tf.float32)\n",
        "    self.rel_encoding=rel_encoding.astype('float32')\n",
        "    self.num_random_walks=num_random_walks\n",
        "    self.len_random_walk=len_random_walk\n",
        "    self.p_val=p_val\n",
        "    self.q_val=q_val\n",
        "    self.n2vemb_size=n2vemb_size\n",
        "    self.prediction_layer = Dense(1,activation=tf.keras.layers.LeakyReLU(), \n",
        "                                  kernel_initializer='glorot_uniform')\n",
        "    if self.flat:\n",
        "        print('one more hidden layer')\n",
        "        self.hidden_layer =  Dense(units, activation=tf.keras.layers.LeakyReLU(),\n",
        "                                   kernel_initializer='glorot_uniform')\n",
        "    else:\n",
        "        self.hidden_layer = None\n",
        "    \n",
        "  def call(self, Feature):\n",
        "      weight_masked=relation_node2vec(self.rel_encoding,self.num_random_walks,self.len_random_walk,\n",
        "                                      self.p_val,self.q_val,self.n2vemb_size) # we are directly using embedding from node2vec\n",
        "      rel_weight=weight_masked\n",
        "      outputs_proped=weight_masked\n",
        "      \n",
        "      if self.flat:\n",
        "          outputs_concated = self.hidden_layer(\n",
        "              tf.concat([Feature, outputs_proped], axis=1))\n",
        "      else:\n",
        "          outputs_concated = tf.concat([Feature, outputs_proped], axis=1)\n",
        "      prediction = self.prediction_layer(outputs_concated)\n",
        "      print('prediction layer input shape: ',outputs_concated.shape)\n",
        "      print('prediction layer output shape: ',prediction.shape)\n",
        "       \n",
        "      return rel_weight,prediction\n",
        "\n",
        "class ReRaLSTM:\n",
        "    def __init__(self, data_path, market_name, tickers_fname, relation_name,\n",
        "                 emb_fname, parameters, depth, loss_name, num_random_walks,len_random_walk,\n",
        "                 p_val, q_val, n2vemb_size, steps=1,\n",
        "                 epochs=50, batch_size=None, flat=False, gpu=False, in_pro=False):\n",
        "    \n",
        "        seed = 123456789\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        tf.random.set_seed(seed)\n",
        "        self.data_path = data_path\n",
        "        self.market_name = market_name\n",
        "        self.tickers_fname = tickers_fname\n",
        "        self.relation_name = relation_name\n",
        "        self.df_loss=pd.DataFrame()\n",
        "        self.depth=depth\n",
        "        self.loss_name=loss_name\n",
        "        self.parameters = copy.copy(parameters)\n",
        "        self.steps = steps\n",
        "        self.epochs = epochs\n",
        "        self.flat = flat\n",
        "        self.inner_prod = in_pro\n",
        "        self.valid_index = 756\n",
        "        self.test_index = 1008\n",
        "        self.fea_dim = 5\n",
        "        self.gpu = gpu\n",
        "        self.num_random_walks=num_random_walks\n",
        "        self.len_random_walk=len_random_walk\n",
        "        self.p_val=p_val\n",
        "        self.q_val=q_val\n",
        "        self.n2vemb_size=n2vemb_size\n",
        "        # load data\n",
        "        self.tickers = np.genfromtxt(os.path.join(data_path, '..', tickers_fname),\n",
        "                                         dtype=str, delimiter='\\t', skip_header=False)\n",
        "        \n",
        "        print('#tickers selected:', len(self.tickers))\n",
        "\n",
        "        # mask_data: mask for time series data, all 1, (num_company,num_days) shape, numpy array\n",
        "        # mask_data is to deal with missing time series data. It will be 0 if there\n",
        "        # is any missing data on a day for a company\n",
        "        # price_data contains normalized price of all days for all stocks. (num_company,num_days)\n",
        "        # gt_data is ground truth or actual daily return. shape (num_company,num_days)\n",
        "        \n",
        "        self.eod_data, self.mask_data, self.gt_data, self.price_data = load_EOD_data(data_path, market_name, self.tickers, steps)\n",
        "        print('price_data shape: ', self.price_data.shape)\n",
        "        print('gt_data shape ', self.gt_data.shape)\n",
        "        \n",
        "        # relation data\n",
        "        rname_tail = {'sector_industry': '_industry_relation.npy',\n",
        "                      'wikidata': '_wiki_relation.npy'}\n",
        "        # rel_encoding: True relations not masked. (num_companies, num_companies, rel_types)\n",
        "        # rel_mask: mask for relation (num_company, num_company).\n",
        "        # If there is a relation the mask will be 0, otherwise, a large negative number there -1e9\n",
        "        if self.relation_name in ['sector_industry','wikidata']:\n",
        "            self.rel_encoding, self.rel_mask = load_relation_data(\n",
        "                    os.path.join(self.data_path,'..', 'relation', self.relation_name,\n",
        "                                 self.market_name + rname_tail[self.relation_name])\n",
        "                    )\n",
        "            # The next part is only relevant if the number of nodes is less than the total\n",
        "            # number of available nodes in the original study. I am assuming that the nodes are \n",
        "            # in the same order in the adjacency matrix as in the ticker file\n",
        "            self.rel_encoding=self.rel_encoding[:self.gt_data.shape[0],:self.gt_data.shape[0],:]\n",
        "            self.rel_mask=self.rel_mask[:self.gt_data.shape[0],:self.gt_data.shape[0]]\n",
        "                    \n",
        "        self.rel_mask = self.rel_mask.astype('float32')\n",
        "        print('relation encoding shape:', self.rel_encoding.shape)\n",
        "        print('relation mask shape:', self.rel_mask.shape)\n",
        "        \n",
        "        # trained pre-trained sequential embedding (num_company, num_days, embedding dimension).\n",
        "        # The last dimension is U or embedding shape\n",
        "        self.embedding = np.load(\n",
        "            os.path.join(data_path, '..', 'pretrain', emb_fname))\n",
        "        print('embedding shape:', self.embedding.shape)\n",
        "        # The next part is only relevant if the number of nodes is less than the total\n",
        "        # number of available nodes in the original study. I am assuming that the nodes are \n",
        "        # in the same order in the adjacency matrix as in the ticker file'''\n",
        "        self.embedding=self.embedding[:self.gt_data.shape[0],:,:] # sequential embedding\n",
        "        \n",
        "        print('embedding shape:', self.embedding.shape)\n",
        "        if batch_size is None:\n",
        "            self.batch_size = len(self.tickers)\n",
        "        else:\n",
        "            self.batch_size = batch_size\n",
        "\n",
        "        self.trade_dates = self.mask_data.shape[1]\n",
        "        self.numCompany = self.rel_mask.shape[0]       \n",
        "        self.model = MyModel(self.numCompany, self.rel_mask, \n",
        "                             self.inner_prod, self.flat,self.rel_encoding, self.num_random_walks,self.len_random_walk,\n",
        "                             self.p_val,self.q_val, self.n2vemb_size, self.parameters['unit'])\n",
        "\n",
        "\n",
        "    def get_batch(self, offset=None):\n",
        "        if offset is None:\n",
        "            offset = random.randrange(0, self.valid_index)\n",
        "        seq_len = self.parameters['seq']\n",
        "        mask_batch = self.mask_data[:, offset: offset + seq_len + self.steps]\n",
        "        mask_batch = np.min(mask_batch, axis=1)\n",
        "        return self.embedding[:, offset, :], np.expand_dims(mask_batch, axis=1), np.expand_dims(\n",
        "                self.price_data[:, offset + seq_len - 1], axis=1), np.expand_dims(\n",
        "                        self.gt_data[:, offset + seq_len + self.steps - 1], axis=1)\n",
        "    def train(self):\n",
        "        seed = 123456789\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        tf.random.set_seed(seed)\n",
        "        if self.gpu == True:\n",
        "            device_name = '/gpu:0'\n",
        "        else:\n",
        "            device_name = '/cpu:0'\n",
        "        print('device name:', device_name) \n",
        "\n",
        "        optimizer = tf.keras.optimizers.Adam()\n",
        "        #train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        " \n",
        "        @tf.function\n",
        "        def train_step(Feature, base_price, ground_truth, mask):\n",
        "          with tf.GradientTape() as tape:\n",
        "            # training=True is only needed if there are layers with different\n",
        "            # behavior during training versus inference (e.g. Dropout).\n",
        "            rel_weight, prediction = self.model(Feature, training=True)\n",
        "            return_ratio = tf.divide(tf.subtract(prediction, base_price), base_price) #(num_company,1) tensor\n",
        "            reg_loss=reg_loss_tgc(ground_truth, return_ratio, mask)\n",
        "            rank_loss=rank_loss_tgc(ground_truth, return_ratio, mask, self)\n",
        "            if self.loss_name=='reg_rank_loss':\n",
        "                loss = reg_loss + tf.cast(parameters['alpha'], tf.float32) * rank_loss\n",
        "            elif self.loss_name=='listnet_loss':\n",
        "                loss= listnet_loss(ground_truth, return_ratio, mask, self)\n",
        "          gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "          print('trainable variables: ', self.model.trainable_variables)\n",
        "          optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "        \n",
        "          '''new code: added rel_weight in return list'''\n",
        "          return rel_weight, loss, reg_loss, rank_loss, return_ratio\n",
        "        \n",
        "        @tf.function\n",
        "        def test_step(Feature, base_price, ground_truth, mask):\n",
        "            # The test step is not doing any further training. It is using the\n",
        "            # model trained in the train_step. training=False\n",
        "            \n",
        "            rel_weight, prediction = self.model(Feature, training=False)\n",
        "            return_ratio = tf.divide(tf.subtract(prediction, base_price), base_price)\n",
        "            reg_loss=reg_loss_tgc(ground_truth, return_ratio, mask)\n",
        "            rank_loss=rank_loss_tgc(ground_truth, return_ratio, mask, self)\n",
        "            if self.loss_name=='reg_rank_loss':\n",
        "                loss = reg_loss + tf.cast(parameters['alpha'], tf.float32) * rank_loss\n",
        "            elif self.loss_name=='listnet_loss':\n",
        "                loss= listnet_loss(ground_truth, return_ratio, mask, self)\n",
        "            return loss, reg_loss, rank_loss, return_ratio\n",
        "\n",
        "\n",
        "        best_valid_pred, best_valid_gt, best_valid_mask=prediction_ground_truth_mask_initializer(\n",
        "                len(self.tickers),\n",
        "                self.test_index - self.valid_index)\n",
        "        best_test_pred, best_test_gt, best_test_mask=prediction_ground_truth_mask_initializer(\n",
        "                len(self.tickers),\n",
        "                self.trade_dates - self.parameters['seq'] -self.test_index - self.steps + 1)\n",
        "        best_valid_loss = np.inf\n",
        "\n",
        "        batch_offsets = np.arange(start=0, stop=self.valid_index, dtype=int)\n",
        "        \n",
        "        '''train on training data'''               \n",
        "        for epoch in range(self.epochs):\n",
        "            t1 = time()\n",
        "            np.random.shuffle(batch_offsets)\n",
        "            tra_loss = 0.0\n",
        "            tra_reg_loss = 0.0\n",
        "            tra_rank_loss = 0.0\n",
        "            for j in range(self.valid_index - self.parameters['seq'] -\n",
        "                                   self.steps + 1):\n",
        "                emb_batch, mask_batch, price_batch, gt_batch = self.get_batch(batch_offsets[j])\n",
        "                \n",
        "                rel_weight, train_cur_loss, train_cur_reg_loss, train_cur_rank_loss, cur_rr= train_step(\n",
        "                        emb_batch, price_batch, gt_batch, mask_batch)\n",
        "                \n",
        "                 \n",
        "                tra_loss += train_cur_loss\n",
        "                tra_reg_loss += train_cur_reg_loss\n",
        "                tra_rank_loss += train_cur_rank_loss\n",
        "\n",
        "            print('Train Loss:',\n",
        "                  tra_loss.numpy() / (self.valid_index - self.parameters['seq'] - self.steps + 1))\n",
        "            \n",
        "            \n",
        "            '''test on validation set'''\n",
        "            cur_valid_pred, cur_valid_gt, cur_valid_mask = prediction_ground_truth_mask_initializer(\n",
        "                    len(self.tickers),\n",
        "                    self.test_index - self.valid_index)\n",
        "            val_loss = 0.0\n",
        "            val_reg_loss = 0.0\n",
        "            val_rank_loss = 0.0\n",
        "            for cur_offset in range(\n",
        "                        self.valid_index - self.parameters['seq'] - self.steps + 1,\n",
        "                        self.test_index - self.parameters['seq'] - self.steps + 1\n",
        "                    ):\n",
        "                emb_batch, mask_batch, price_batch, gt_batch = self.get_batch(\n",
        "                            cur_offset)\n",
        "                # using test_step to get the validation loss\n",
        "                val_cur_loss, val_cur_reg_loss, val_cur_rank_loss, cur_rr = test_step(emb_batch, price_batch, gt_batch, mask_batch)\n",
        "                val_loss += val_cur_loss\n",
        "                val_reg_loss += val_cur_reg_loss\n",
        "                val_rank_loss += val_cur_rank_loss\n",
        "        \n",
        "                cur_valid_pred[:, cur_offset - (self.valid_index -\n",
        "                            self.parameters['seq'] - self.steps + 1)] = copy.copy(cur_rr[:, 0])\n",
        "                cur_valid_gt[:, cur_offset - (self.valid_index -\n",
        "                            self.parameters['seq'] - self.steps + 1)] = copy.copy(gt_batch[:, 0]) \n",
        "                cur_valid_mask[:, cur_offset - (self.valid_index -\n",
        "                            self.parameters['seq'] - self.steps + 1)] = copy.copy(mask_batch[:, 0])\n",
        "            print('Valid loss:',\n",
        "                          val_loss.numpy()  / (self.test_index - self.valid_index))\n",
        "          \n",
        "            \n",
        "            '''test on testing set'''\n",
        "            cur_test_pred,cur_test_gt,cur_test_mask = prediction_ground_truth_mask_initializer(\n",
        "                    len(self.tickers),\n",
        "                    self.trade_dates - self.test_index)\n",
        "            test_loss = 0.0\n",
        "            test_reg_loss = 0.0\n",
        "            test_rank_loss = 0.0\n",
        "            for cur_offset in range(\n",
        "                    self.test_index - self.parameters['seq'] - self.steps + 1,\n",
        "                    self.trade_dates - self.parameters['seq'] - self.steps + 1):\n",
        "                emb_batch, mask_batch, price_batch, gt_batch = self.get_batch(\n",
        "                            cur_offset) # sequential_embedding, mask for time series data, price data and ground truth data\n",
        "                # using test step to get the test loss for current epoch\n",
        "                test_cur_loss, test_cur_reg_loss, test_cur_rank_loss, cur_rr = test_step(emb_batch, price_batch, gt_batch, mask_batch)\n",
        "        \n",
        "                test_loss += test_cur_loss\n",
        "                test_reg_loss += test_cur_reg_loss\n",
        "                test_rank_loss += test_cur_rank_loss\n",
        "        \n",
        "                cur_test_pred[:, cur_offset - (self.test_index -\n",
        "                            self.parameters['seq'] - self.steps + 1)] = copy.copy(cur_rr[:, 0])\n",
        "                cur_test_gt[:, cur_offset - (self.test_index -\n",
        "                            self.parameters['seq'] - self.steps + 1)] = copy.copy(gt_batch[:, 0])\n",
        "                cur_test_mask[:, cur_offset - (self.test_index -\n",
        "                            self.parameters['seq'] - self.steps + 1)] = copy.copy(mask_batch[:, 0])\n",
        "            print('Test loss:',\n",
        "                          test_loss.numpy() / (self.trade_dates - self.test_index))\n",
        "            if val_loss / (self.test_index - self.valid_index) < best_valid_loss:\n",
        "                     best_valid_loss = val_loss.numpy() / (self.test_index - self.valid_index)\n",
        "                     best_valid_gt = copy.copy(cur_valid_gt)\n",
        "                     best_valid_pred = copy.copy(cur_valid_pred)\n",
        "                     best_valid_mask = copy.copy(cur_valid_mask)\n",
        "                     best_test_gt = copy.copy(cur_test_gt)\n",
        "                     best_test_pred = copy.copy(cur_test_pred)\n",
        "                     best_test_mask = copy.copy(cur_test_mask)\n",
        "                     print('Better valid loss:', best_valid_loss)\n",
        "            '''Calculate the evaluation performance after certain epochs. If epoch==15000, use it 50 or 100'''\n",
        "            if epoch%10==0:\n",
        "                cur_valid_perf = evaluate(cur_valid_pred, cur_valid_gt, cur_valid_mask)\n",
        "                print('\\t Valid preformance:', cur_valid_perf)\n",
        "                cur_test_perf = evaluate(cur_test_pred, cur_test_gt, cur_test_mask)\n",
        "                print('\\t Test performance:', cur_test_perf)\n",
        "                self.df_loss=make_df_loss(self, epoch, cur_valid_perf, cur_test_perf, tra_loss, tra_reg_loss,\n",
        "                                          val_loss, test_loss)\n",
        "                self.df_loss.to_csv('df_loss_node2vec_'+self.loss_name+'_'+self.market_name+'_'+\n",
        "                                    self.relation_name+'_'+\n",
        "                                    str(self.epochs)+'_epochs_'+str(RR_LSTM.num_random_walks)+'_num_random_walks_'+\n",
        "                                    str(RR_LSTM.len_random_walk)+'_len_random_walk_'+str(RR_LSTM.p_val)+'_p_val_'+\n",
        "                                    str(RR_LSTM.q_val)+'_q_val_'+str(RR_LSTM.n2vemb_size)+'_n2vemb_size'+'.csv',index=False)\n",
        "            t4 = time()\n",
        "            print('epoch:', epoch, ('time: %.4f ' % (t4 - t1)))\n",
        "        \n",
        "                \n",
        "        # The function is returning model in addition to other stats\n",
        "        return self.model, best_valid_pred, best_valid_gt, best_valid_mask, best_test_pred, best_test_gt, best_test_mask\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    desc = 'train a relational rank lstm model'\n",
        "    parser = argparse.ArgumentParser(description=desc)\n",
        "    parser.add_argument('-paths', help='path of EOD data',\n",
        "                        default='/content/stock-ranking-using-list-wise-approach/data')\n",
        "    parser.add_argument('-m', help='market name', default='NASDAQ')\n",
        "    parser.add_argument('-ls',default='reg_rank_loss', \n",
        "                        help='listnet_loss or reg_rank_loss')\n",
        "    parser.add_argument('-t', help='fname for selected tickers')\n",
        "    parser.add_argument('-l', default=4,\n",
        "                        help='length of historical sequence for feature')\n",
        "    parser.add_argument('-u', default=64,\n",
        "                        help='number of hidden units in lstm')\n",
        "    parser.add_argument('-s', default=1,\n",
        "                        help='steps to make prediction')\n",
        "    parser.add_argument('-r', default=0.001,\n",
        "                        help='learning rate')\n",
        "    parser.add_argument('-a', default=1,\n",
        "                        help='alpha, the weight of ranking loss')\n",
        "    parser.add_argument('-g', '--gpu', type=int, default=0, help='use gpu')\n",
        "\n",
        "    parser.add_argument('-e', '--emb_file', type=str,\n",
        "                        default='NASDAQ_rank_lstm_seq-16_unit-64_2.csv.npy',\n",
        "                        help='fname for pretrained sequential embedding') #NASDAQ_rank_lstm_seq-16_unit-64_2.csv.npy#NYSE_rank_lstm_seq-8_unit-32_0.csv.npy\n",
        "    parser.add_argument('-rn', '--rel_name', type=str,\n",
        "                        default='wikidata',\n",
        "                        help='relation type: sector_industry or wikidata')\n",
        "    parser.add_argument('-ip', '--inner_prod', type=int, default=1)\n",
        "    parser.add_argument('-depth', type=int, default=5)\n",
        "    parser.add_argument('-epoch_num', type=int, default=15)\n",
        "    parser.add_argument('-num_rw',type=int,default=20)\n",
        "    parser.add_argument('-len_rw',type=int, default=8)\n",
        "    parser.add_argument('-p_val',type=int, default=1)\n",
        "    parser.add_argument('-q_val',type=int, default=1)\n",
        "    parser.add_argument('-n2vemb_size',type=int, default=64)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.t is None:\n",
        "        args.t = args.m + '_tickers_qualify_dr-0.98_min-5_smooth.csv'\n",
        "    args.gpu = (args.gpu == 1)\n",
        "\n",
        "    args.inner_prod = (args.inner_prod == 1)\n",
        "\n",
        "    parameters = {'seq': int(args.l), 'unit': int(args.u), 'lr': float(args.r),\n",
        "                  'alpha': float(args.a)}\n",
        "    print('arguments:', args)\n",
        "    print('parameters:', parameters)\n",
        "\n",
        "    RR_LSTM = ReRaLSTM(\n",
        "        data_path=args.paths,\n",
        "        market_name=args.m,\n",
        "        tickers_fname=args.t,\n",
        "        relation_name=args.rel_name,\n",
        "        emb_fname=args.emb_file,\n",
        "        parameters=parameters,\n",
        "        steps=1, epochs=args.epoch_num, batch_size=None, gpu=args.gpu,\n",
        "        in_pro=args.inner_prod, depth=args.depth,\n",
        "        loss_name=args.ls,\n",
        "        num_random_walks=args.num_rw,\n",
        "        len_random_walk=args.len_rw,\n",
        "        p_val=args.p_val,\n",
        "        q_val=args.q_val,\n",
        "        n2vemb_size=args.n2vemb_size\n",
        "    )\n",
        "    pred_all = RR_LSTM.train() \n",
        "    df_loss=RR_LSTM.df_loss\n",
        "    df_loss.to_csv('df_loss_node2vec_'+RR_LSTM.loss_name+'_'+RR_LSTM.market_name+'_'+\n",
        "                   RR_LSTM.relation_name+'_'+\n",
        "                   str(RR_LSTM.epochs)+'_epochs_'+str(RR_LSTM.num_random_walks)+'_num_random_walks_'+\n",
        "                   str(RR_LSTM.len_random_walk)+'_len_random_walk_'+str(RR_LSTM.p_val)+'_p_val_'+\n",
        "                   str(RR_LSTM.q_val)+'_q_val_'+str(RR_LSTM.n2vemb_size)+'_n2vemb_size'+'.csv',index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "PVe8FBceaukh",
        "outputId": "62ee007f-2323-4b8a-d507-647b52c66ae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] [-paths PATHS] [-m M] [-ls LS] [-t T] [-l L]\n",
            "                             [-u U] [-s S] [-r R] [-a A] [-g GPU]\n",
            "                             [-e EMB_FILE] [-rn REL_NAME] [-ip INNER_PROD]\n",
            "                             [-depth DEPTH] [-epoch_num EPOCH_NUM]\n",
            "                             [-num_rw NUM_RW] [-len_rw LEN_RW] [-p_val P_VAL]\n",
            "                             [-q_val Q_VAL] [-n2vemb_size N2VEMB_SIZE]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-18624252-679e-4554-bc6d-b29167a25627.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !tar -xvf  '/content/stock-ranking-using-list-wise-approach/data/relation.tar.gz' -C 'content/cell_images'\n",
        "!tar -xzvf \"/content/stock-ranking-using-list-wise-approach/data/\" \"/content/stock-ranking-using-list-wise-approach/data/relation.tar.gz\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-B5AxAJVbtRF",
        "outputId": "68ac53ef-a465-41e8-af62-f03ecc7ce25f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar (child): /content/stock-ranking-using-list-wise-approach/data/: Cannot read: Is a directory\n",
            "tar (child): At beginning of tape, quitting now\n",
            "tar (child): Error is not recoverable: exiting now\n",
            "\n",
            "gzip: stdin: unexpected end of file\n",
            "tar: Child returned status 2\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install stellargraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbIvfedjbOPj",
        "outputId": "97f33315-ddec-48b4-ad8f-90c9892a6363"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stellargraph\n",
            "  Downloading stellargraph-1.2.1-py3-none-any.whl (435 kB)\n",
            "\u001b[?25l\r\u001b[K     |                               | 10 kB 19.4 MB/s eta 0:00:01\r\u001b[K     |                              | 20 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |                             | 30 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |                             | 40 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |                            | 51 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |                           | 61 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |                          | 71 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |                          | 81 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |                         | 92 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |                        | 102 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |                       | 112 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |                       | 122 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |                      | 133 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |                     | 143 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |                    | 153 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |                    | 163 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |                   | 174 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |                  | 184 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |                 | 194 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |                 | 204 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |                | 215 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |               | 225 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |              | 235 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |              | 245 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |             | 256 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |            | 266 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |           | 276 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |           | 286 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |          | 296 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |         | 307 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |        | 317 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |        | 327 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |       | 337 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |      | 348 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |     | 358 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |     | 368 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |    | 378 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |   | 389 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |  | 399 kB 5.0 MB/s eta 0:00:01\r\u001b[K     | | 409 kB 5.0 MB/s eta 0:00:01\r\u001b[K     | | 419 kB 5.0 MB/s eta 0:00:01\r\u001b[K     || 430 kB 5.0 MB/s eta 0:00:01\r\u001b[K     || 435 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (3.2.2)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (3.6.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (1.0.2)\n",
            "Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (2.7.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (2.6.3)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (1.19.5)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (1.1.5)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->stellargraph) (5.2.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->stellargraph) (1.15.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->stellargraph) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->stellargraph) (3.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->stellargraph) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->stellargraph) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->stellargraph) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->stellargraph) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->stellargraph) (3.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (1.43.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (0.23.1)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (0.37.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (3.10.0.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (3.17.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (3.1.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (12.0.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (1.0.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (2.7.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (1.13.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (1.1.2)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (0.4.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (2.7.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (2.7.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.1.0->stellargraph) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (0.6.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (2.23.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (3.1.1)\n",
            "Installing collected packages: stellargraph\n",
            "Successfully installed stellargraph-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stellargraph import StellarGraph\n",
        "import networkx as nx\n",
        "from stellargraph.data import BiasedRandomWalk\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "def relation_node2vec(relation_encoding_org,num_random_walks,len_random_walk,p_val,q_val,n2vemb_size):\n",
        "    relation_encoding_2d_weighted=relation_encoding_org.sum(axis=2) # weighted 2d graph with sum\n",
        "    relation_encoding_2d_unweighted=(relation_encoding_2d_weighted>0).astype(int) # unweighted 2d graph\n",
        "    G_nx=nx.from_numpy_array(relation_encoding_2d_unweighted) # loading as networkx graph\n",
        "    G = StellarGraph.from_networkx(G_nx)\n",
        "    print(G.info())\n",
        "    # Corpus generation using random walks\n",
        "    rw = BiasedRandomWalk(G)\n",
        "    walks = rw.run(\n",
        "            nodes=list(G.nodes()),  # root nodes\n",
        "            length=len_random_walk,  # maximum length of a random walk\n",
        "            n=num_random_walks,  # number of random walks per root node\n",
        "            p=p_val,  # Defines (unormalised) probability, 1/p, of returning to source node\n",
        "            q=q_val,  # Defines (unormalised) probability, 1/q, for moving away from source node\n",
        "            )\n",
        "    print(\"Number of random walks: {}\".format(len(walks))) # total number for all nodes in the graph\n",
        "    # Representation Learning using Word2Vec\n",
        "    str_walks = [[str(n) for n in walk] for walk in walks]\n",
        "    # size: Dimensionality of the output\n",
        "    # window: Maximum distance between the current and predicted word within a sentence\n",
        "    # min_count : Ignores all words with total frequency lower than this\n",
        "    # sg : {0, 1}, Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
        "    # iter : Number of iterations (epochs) over the corpus.\n",
        "    # workers : Use these many worker threads to train the model (=faster training with multicore machines)\n",
        "    model = Word2Vec(str_walks, size=n2vemb_size, window=5, min_count=0, sg=1, workers=2, iter=1)\n",
        "    # Retrieve node embeddings and corresponding subjects\n",
        "    node_embeddings = (model.wv.vectors)  # numpy.ndarray of size number of nodes times embeddings dimensionality\n",
        "    print(node_embeddings.shape)\n",
        "    return node_embeddings"
      ],
      "metadata": {
        "id": "WrPyWLRqa1ZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FOP6tlVXbM19"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}